
# ğŸ“ LLaMA FastAPI Text Summarizer

A local AI-powered text summarizer that runs 100% on your machine.
No internet required. No data leaves your computer.

---
## ğŸ–¼ï¸ Home Screenshot

> ğŸ’¡![Aprication Screen](images/app-home-screen.png)

-----

### **Built With:**
- **FastAPI**: Backend API
- **Streamlit**: Frontend UI
- **Ollama + LLaMA3**: Local large language model (LLM)
- **Private & Secure**: Ideal for sensitive or confidential content

Perfect for summarizing articles, emails, reports, and more â€” all offline.

---

### **ğŸš€ Quick Start (How to Run)**

#### **1. Prerequisites**

Before running, install:
- [Python 3.9+](https://www.python.org/downloads/)
- [Ollama](https://ollama.com/download) (for local LLM)
- [Git](https://git-scm.com/downloads) (optional, for package install)

> ğŸ’¡ This app works on **Windows, macOS, and Linux**.

#### **2. Setup the Project**

```bash
# Clone the project (or download as ZIP)
git clone [https://github.com/sabelogumede/LLaMA-FastAPI-Summarizer.git](https://github.com/sabelogumede/LLaMA-FastAPI-Summarizer.git)
cd LLaMA-FastAPI-Summarizer

# Create a virtual environment
python -m venv venv

# Activate it
# On Windows:
venv\Scripts\activate
# On macOS/Linux:
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
````

#### **3. Download the LLM Model**

Run this in your terminal:

```bash
# Start Ollama in the background (opens a new window)
ollama serve

# In another terminal:
ollama pull llama3
```

> âœ… This downloads the LLaMA3 model (\~4.7GB). This step only needs to be done once.

#### **4. Run the App**

Open **two terminal windows**:

##### **Terminal 1: Start the FastAPI Backend**

```bash
uvicorn backend.main:app --host 0.0.0.0 --port 8000 --reload
```

##### **Terminal 2: Start the Streamlit Frontend**

```bash
streamlit run frontend/app.py
```

âœ… Open the app at: [http://localhost:8501](https://www.google.com/search?q=http://localhost:8501)

-----

### **ğŸ§± Architecture Overview**

The application's architecture is a simple, effective pipeline:

> ![Architecture Diagram](images/app-architecture.png)

- **User**: Interacts with the Streamlit UI

  - **Frontend**: `frontend/app.py` â€“ Streamlit interface
  - **Backend**: `backend/main.py` â€“ FastAPI server
  - **Model**: Runs locally via Ollama (`llama3`)
  - **Communication**: JSON over HTTP (localhost only)

ğŸ” **No data ever leaves your machine, ensuring complete privacy.**

-----

## API EndPonits

  * **Get**: **/health** A simple health check to verify the application is running.
  * **Post**: **/summarize** summarize the given text input.

-----

### **ğŸ› ï¸ Features**

  - âœ… Live word & character count on input
  - âœ… Clear input after summarization
  - âœ… Input validation (blocks short inputs)
  - âœ… Error handling (network, timeout, empty responses)
  - âœ… Responsive and user-friendly UI

-----

### **âš™ï¸ Requirements**

See `requirements.txt`. Key packages include:

  - `fastapi` + `uvicorn`: API backend
  - `streamlit`: Frontend
  - `requests`: For calling Ollama
  - `pydantic`: Data validation
  - `streamlit-copy-button`: For copy functionality (installed from GitHub)

> ğŸ’¡ The `streamlit-copy-button` package is not on PyPI. It's installed directly from GitHub with the following command:
>
> ```bash
> pip install git+[https://github.com/jandot/streamlit-copy-button.git](https://github.com/jandot/streamlit-copy-button.git)
> ```

-----

### **ğŸ“‚ Project Structure**

```
LLaMA-FastAPI-Summarizer/
â”‚
â”œâ”€â”€ .gitignore               # Ignores venv, cache, IDE files
â”œâ”€â”€ README.md                # This file
â”œâ”€â”€ requirements.txt         # Python dependencies
â”‚
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py              # FastAPI summarization endpoint
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py               # Streamlit UI with live counts & copy
â”‚
â””â”€â”€ venv/                    # (Ignored) Python virtual environment
```

-----

### **ğŸ“Œ Tips**

  - ğŸ¢ **Summarization takes 10â€“60 seconds** â€” please be patient\!
  - ğŸ“ Short inputs (\<5 words) are rejected to prevent the AI from "explaining" instead of summarizing.
  - ğŸ”„ Use the **"ğŸ—‘ï¸ Clear Input"** button to start over.
  - ğŸ“‹ Click **"ğŸ“‹ Copy summary"** to copy results to your clipboard.

-----

### **ğŸ“¦ Future Ideas**

  - Export summary as `.txt`
  - Support PDF/TXT file uploads
  - Add a model selector (e.g., `llama3`, `mistral`)
  - Add summary length options (short/medium/long)
  - Dockerize for easy deployment

-----

### **ğŸ™Œ Made with â¤ï¸**

By Sabelo Gumede â€“ for local, private, and powerful AI.

Keep building\! ğŸš€

```
```